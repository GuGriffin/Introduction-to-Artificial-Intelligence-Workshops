{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: GenerativeAI Chatbots\n",
    "\n",
    "This week you will be learning about how about how to generate text with large language models (LLMs). \n",
    "\n",
    "This code uses the excellent [huggingface transformers library](https://huggingface.co/docs/transformers/en/index), there are many LLMs and other transformer-based models available in this library. \n",
    "\n",
    "Today you will be using [huggingface's SmolLM](https://huggingface.co/blog/smollm), an LLM which has a small parameter count, which means it does not use much space or memory on your PC to run, and can be run efficiently on low-powered consumer hardware like laptops. SmolLM was trained on [a curated dataset of educational and synthetically generated text](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus) designed specifically for conversational instruct-based LLMs.\n",
    "\n",
    "Before you get started though, let just make sure that this notebook is setup to run using the `nlp` conda environment that you created at the start of term.\n",
    "\n",
    "To set this notebook to the right environment, click the **Select kernel** button in the top right corner of this notebook, then select **Python Environments...** and then select the environment `nlp`.\n",
    "\n",
    "To double check you have done this correctly, hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can import the libraries you will need:\n",
    "\n",
    "(this cell also uses [environment variables](https://docs.python.org/3/using/cmdline.html#environment-variables) to suppress warnings and give other instructions to the huggingface transformers library to prevent it printing unwanted messages to the terminal and disrupting the flow of conversation from the chatbot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different types of LLM\n",
    "\n",
    "In this notebook you will see code to run two different versions of the SmolLM model.\n",
    "\n",
    "The first is the [standard LLM](#standard-llm) model that predicts the next token in a sequence without being conditioned on prompt instructions. \n",
    "\n",
    "The second is the [Instruct-LLM](#instruct-llm) model that has been fine-tuned on an instruct dataset. This allows it to be given prompt instructions that it responds to appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard LLM\n",
    "\n",
    "This cell loads in the standard [SmolLM-135M](https://huggingface.co/HuggingFaceTB/SmolLM-135M) model weights and generates text based on a prompt. This standard LLM is useful for completing text based on a prompt, for instance finish a story or song lyrics (these types of models are also used in email-autocomplete and code co-pilot autocomplete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "\n",
    "device = \"cpu\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "input_str = \"There once was a man from Nantucket\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "inputs = tokenizer.encode(input_str, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.5, top_p=0.99, min_p=0.1, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the `input_str` to get generations based on different inputs. Try change the parameters `temperature` `top_p` `min_p` and `max_new_tokens` to alter the randomness and length of the generated output.\n",
    "\n",
    "Try comparing the results from the same prompt in this model and with the [Instruct-LLM](#instruct-llm) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct LLM\n",
    "\n",
    "This cell loads in the [Instruct-SmolLM-135M](https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct) model weights and generates text based on a prompt. This instruct LLM has been fine-tuned on an instruct dataset. This conditions the model to answer questions and perform tasks based on prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "user_prompt_str = \"How old is Taylor Swift?\"\n",
    "system_prompt_str = \"You are a friendly chatbot that answers questions in single sentences.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": system_prompt_str,\n",
    "    },\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": user_prompt_str\n",
    "    }]\n",
    "\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "input_token_len = inputs.shape[-1]\n",
    "outputs = model.generate(inputs, max_new_tokens=100, temperature=0.5, top_p=0.99, min_p=0.1, do_sample=True)\n",
    "out_str = tokenizer.decode(outputs[0][input_token_len:])\n",
    "print(out_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to remove the instruct formatting from the generated_output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_str = re.sub(r'(<\\|im_start\\|>assistant\\n)|(<\\|im_end\\|>)','',out_str)\n",
    "print(out_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try changing the `user_prompt_str` to get generations based on different inputs. To alter the behaviour of the model when generating the output try altering the `system_prompt_str`.\n",
    "\n",
    "Try change the parameters `temperature` `top_p` `min_p` and `max_new_tokens` to alter the randomness and length of the generated output.\n",
    "\n",
    "How do the results from this model compare to the same prompt in the [Standard LLM](#standard-llm) model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting LLMs for different kinds of chatbot\n",
    "\n",
    "Go to the file [week-8b-instruct-LLM-chatbot.py](week-8b-instruct-LLM-chatbot.py), here is all of the code already generated for a simple wrapper around the [Instruct-LLM](#instruct-llm) chatbot. \n",
    "\n",
    "There are a number of tasks you could do to build on this code. Try these in any order, and if you think you could use this code in your chatbot for your project, experiment with customising it to best fit with your project.\n",
    "\n",
    "**Task A:** Try changing the system prompt and other hyper-parameters to get the chatbot to behave in a different way, with either a different personality or functionality.\n",
    "\n",
    "**Task B:** Feed local data or data from the web into the prompt (this could be in both the user or system prompt) to make a basic retrieval-augmented-generation (RAG) chatbot.\n",
    "\n",
    "**Task C:** Write scaffolding around the code to use the instruct LLM for a specific task, this could be:\n",
    "- Using an LLM as a partner in an interactive game\n",
    "- Using the LLM to write parts of a story \n",
    "- Using an LLM to summarise text\n",
    "- Using an LLM to make suggestions based on a user input\n",
    "\n",
    "**Task D:** Integrate this LLM into another chatbot. Can you combine this with a retrieval based chatbot? an intent based chatbot? or a rule-based chatbot with only uses the LLM at specific times?\n",
    "\n",
    "**Task E:** Create a chatbot that uses a [standard LLM model](#standard-llm) instead of an instruct LLM. Think about what kinds of tasks a standard LLM might be better at."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
