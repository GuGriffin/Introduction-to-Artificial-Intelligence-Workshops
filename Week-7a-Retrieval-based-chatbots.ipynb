{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7a: Retrieval based chatbots\n",
    "\n",
    "This week you will be learning about methods to help you build chatbots that retrieve data, both from local datasets ([Part 1](#part-1-retrieving-local-data)) and from the world wide web ([Part 2](#part-2-retrieving-data-from-the-web)). These two parts are independent so you can do part 2 before part 1 if you are more interested in web scraping.\n",
    "\n",
    "Before you get started though, let just make sure that this notebook is setup to run using the `nlp` conda environment that you created last week.\n",
    "\n",
    "To set this notebook to the right environment, click the **Select kernel** button in the top right corner of this notebook, then select **Python Environments...** and then select the environment `nlp`.\n",
    "\n",
    "To double check you have done this correctly, hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import libraries\n",
    "\n",
    "Now test that the libraries we will be using today for part 1 are installed correctly and can be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Retrieving local data\n",
    "\n",
    "In this section you will at how to compare text inputs (aka queries) to documents in a local dataset. You will be trying to find the most relevant songs by comparing a text query to a dataset of song lyrics. Most of the code is already done for you, [the tasks](#task-2-add-functions-to-chatbot) are mostly getting you to incorporate this code into a helpful retrieval based chatbot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset into a pandas dataframe\n",
    "\n",
    "Here is some code that loads in a dataset of song lyrics from the TSV file [class-datasets/lyric_data.tsv](class-datasets/lyric_data.tsv) into a [Pandas dataframe](https://www.w3schools.com/python/pandas/pandas_dataframes.asp), which is a very useful way of storing data tables in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics(file_path, separator, column_names):\n",
    "    df = pd.read_csv(file_path, sep=separator, usecols=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Song lyrics to bag of words\n",
    "\n",
    "This function will convert the song lyrics into a bag of words (BoW) matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyrics_to_bow(df):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    lyrics_matrix = vectorizer.fit_transform(df['LYRICS'])\n",
    "    return vectorizer, lyrics_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". The word *matrix* sounds very technical, but it is essential just a big table that looks something like this:\n",
    "\n",
    "![bag of words matrix visualisation](media/week-6/bow-visualisation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find nearest song function\n",
    "\n",
    "The following function takes an text query as input, vectorises it using the vectorizer that is configured to the vocabulary of the dataset of song lyrics, then uses a function called the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) (more on the maths behind this in term 2!) to measure how similiar the BoW vector that represents our input query is to each BoW vector for every document (song's lyrics) in our dataset.\n",
    "\n",
    "It then returns the artist name, song name and the lyrics for the closest matching song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the nearest song based on matrix multiplication\n",
    "def find_nearest_song(input_query, vectorizer, lyrics_matrix, df):\n",
    "    \n",
    "    # Use the same vectorizer as the lyrics so that the dimensions match (so that they use the same dictionary of words)\n",
    "    input_bow = vectorizer.transform([input_query])\n",
    "    \n",
    "    # Get similarity between input query and output \n",
    "    similarities = cosine_similarity(input_bow, lyrics_matrix).flatten()\n",
    "    \n",
    "    # Find the index of the most similar song\n",
    "    nearest_index = similarities.argmax()\n",
    "    \n",
    "    # Extract artist, song name and the lyrics\n",
    "    artist_song_id = df.iloc[nearest_index]['ARTIST_NAME-SONG_NAME']\n",
    "    artist_id, song_id = artist_song_id.split('-')\n",
    "    lyrics = df.iloc[nearest_index]['LYRICS']\n",
    "    \n",
    "    return artist_id, song_id, lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and vectorise dataset\n",
    "\n",
    "Now you can load in your data set and vectorise it, you should end up with a *\"Compressed Sparse Row sparse matrix of dtype 'int64'\"*, which essentially is just a table of integers that mostly has 0s in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'class-datasets/lyric_data.tsv'\n",
    "column_names = ['ARTIST_NAME-SONG_NAME', 'SONG_NAME', 'LYRICS']\n",
    "separator = '\\t'\n",
    "\n",
    "df = load_lyrics(dataset_path, separator, column_names)\n",
    "vectorizer, lyrics_matrix = lyrics_to_bow(df)\n",
    "lyrics_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test an input, try changing the value in `input_query` to find different songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"sunshine and rainbows\"\n",
    "artist_id, song_id, nearest_lyrics = find_nearest_song(input_query, vectorizer, lyrics_matrix, df)\n",
    "\n",
    "print(f\"Nearest Song: {song_id} by {artist_id}\")\n",
    "print(f\"Lyrics: {nearest_lyrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Use TF-IDF instead of Bag of Words\n",
    "\n",
    "Write a new function that uses [the TfidfVectorizer](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) instead of [the CountVectorizer](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) (aka bag of words).\n",
    "\n",
    "Use the code in [the song lyrics to bag of words function](#song-lyrics-to-bag-of-words) as inspiration. No need to import TfidfVectorizer as that has [already been imported](#import-libraries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyrics_to_tfidf(df):\n",
    "    # You're code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now replace the BoW vectorize and matrix with the new tf_idf one, you should end up with a *\"Compressed Sparse Row sparse matrix of dtype 'float64'\"*, this is now a table of **Floats** that mostly has 0s in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, lyrics_matrix = lyrics_to_tfidf(df)\n",
    "lyrics_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now test this code again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"sunshine and rainbows\"\n",
    "artist_id, song_id, nearest_lyrics = find_nearest_song(input_query, vectorizer, lyrics_matrix, df)\n",
    "\n",
    "print(f\"Nearest Song: {song_id} by {artist_id}\")\n",
    "print(f\"Lyrics: {nearest_lyrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Add functions to Chatbot\n",
    "\n",
    "In week-6b-music-info-chatbot.py add the functions `load_lyrics`, `lyrics_to_bow`, `lyrics_to_tfidf` and `find_nearest_song` from here to the class `MusicBot` as member functions, don't forget to use the `self` keyword here. \n",
    "\n",
    "### Task 3: Load and pre-process data in constructor\n",
    "\n",
    "Then include the code for [loading and vectorizing your data](#load-and-vectorise-dataset) into the constructor (`__init__`) of your the music chatbot. Make sure that you assign the variables you create as member variables to your class.\n",
    "\n",
    "### Task 4: Create chat interface for getting song recommendations\n",
    "\n",
    "In the function `generate_response`:\n",
    "1. Use a regex to match the string input 'give me a song about {x}' or 'recommend me a song about {x}', where {x} is whatever collection of words that a user is looking for a song about. \n",
    "2. Pass the extracted group from *{x}* as the query into the function `find_nearest_song`\n",
    "3. Return a response that includes the artist name and the song name, and a sample of the lyrics (i.e. the first 250 characters).\n",
    "\n",
    "## Bonus tasks\n",
    "\n",
    "Here are some bonus tasks related to the first part of this notebook. Feel free to move straight onto [part 2 of the notebook](#part-2-retrieving-data-from-the-web) and come back to these if you want more of a challenge.\n",
    "\n",
    "#### Bonus task A\n",
    "\n",
    "Can you use a [stemmer](https://www.nltk.org/howto/stem.html) or a [lemmatizer](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet) from the [NLTK library](https://www.nltk.org/index.html) in your chatbot? \n",
    "\n",
    "> Tips: \n",
    "> 1. Write a function for stemming or lemmatising your text.\n",
    "> 2. It is a good idea to write and test this code in this notebook first before implementing it in your chatbot.\n",
    "> 3. Step 5 from this [stackoverflow answer](https://stackoverflow.com/a/45670652) shows you how to best apply stemming to text in the way that the song lyrics are formatted.\n",
    "> 4. To transform all the lyrics in your dataframe, you can use pass your stemming/lemmatiser function as a parameter into the [dataframe.apply()](https://www.w3schools.com/python/pandas/ref_df_apply.asp) class method to quickly transform all of the song lyrics in one go.\n",
    "\n",
    "#### Bonus task B\n",
    "\n",
    "Can you include adapt the code to include the lyrics of Taylor Swift songs from [class-datasets/TaylorSwift.csv](class-datasets/TaylorSwift.csv)? \n",
    "\n",
    "> Tips: The data is in a different format and uses different names for the columns. You will need to adapt the previous code to take account of this:\n",
    "> Simple solution: If you only want taylor swift songs you can just change these values. \n",
    "> Advanced solution: Writing code that loads both datasets in and harmonises them before calculating the BoW or TF-IDF matrix will be more complicated. You will need to [rename](https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas) and [reorder](https://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns) the columns of the dataframes to be the same before [concatonating](https://stackoverflow.com/questions/59267129/how-to-concatenate-multiple-dataframes-from-multiple-sources-in-pandas) them.\n",
    "\n",
    "#### Bonus task C\n",
    "\n",
    "Have a look for [other datasets of song lyrics on kaggle](https://www.kaggle.com/search?q=song+lyrics+in%3Adatasets), can you adapt the code to work with any of these bigger datasets? \n",
    "> **Warning:** Loading and calculating your BoW or TF-IDF matrix may become very slow if your dataset is very large! If this happens, try [dropping some of the rows from your dataframe](https://stackoverflow.com/a/77145573) before vectorizing if this becomes a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Retrieving data from the web\n",
    "\n",
    "In part two you will be looking at how to extract data from the web using some simple web scraping. You will be extracting some biographical data from wikipedia for musicians that the user inputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import the libraries we need for this and make sure they can be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Limit requests \n",
    "\n",
    "Run this code to limit the rate and number of requests that are made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit requests, code from: https://stackoverflow.com/a/47475019\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get html content function\n",
    "\n",
    "Function that gets data from a URL and uses beautiful soup to extract the html content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(target_url):\n",
    "    try:\n",
    "        r = session.get(target_url)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_url = 'https://en.wikipedia.org/wiki/Taylor_Swift'\n",
    "soup = get_html_content(target_url)\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract biographical information from the HTML\n",
    "\n",
    "Extract name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = soup.find(\"div\", class_=\"fn\").get_text(strip=True)\n",
    "print(f\"Name: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract date of birth and age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_info = soup.find(\"span\", class_=\"bday\")\n",
    "birth_date = birth_info.get_text(strip=True)\n",
    "age = soup.find(\"span\", class_=\"ForceAgeToShow\").get_text(strip=True).replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "print(f\"Birth Date: {birth_date}\")\n",
    "print(f\"Age: {age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract birthplace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthplace = soup.find(\"div\", class_=\"birthplace\").get_text(strip=True)\n",
    "\n",
    "print(f\"Birthplace: {birthplace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract musical genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_list = soup.find(\"th\", string=\"Genres\").find_next_sibling(\"td\").find_all(\"li\")\n",
    "genres = [genre.get_text(strip=True) for genre in genres_list]\n",
    "\n",
    "print(f\"Genres: {', '.join(genres)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Format URLs\n",
    "\n",
    "Write a function that takes a string of a musicians name and turns it into the name of the page on wikipedia.\n",
    "\n",
    "For instance: 'taylor swift' -> 'Talyor_Swift'\n",
    "\n",
    "To do this you will need to [capitalise each word in the string](https://stackoverflow.com/a/12336911), strip any whitespace at the [start](https://www.w3schools.com/python/ref_string_lstrip.asp) or [end](https://www.w3schools.com/python/ref_string_rstrip.asp) of the string, and replace spaces in between parts of the name with an underscore. You can do this with the Python [string replace method](https://www.w3schools.com/python/ref_string_replace.asp) or the [regex sub method](https://docs.python.org/3/library/re.html#re.sub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_wikipedia_page_name(input_str):\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the code with the following function to see if a URL exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from here: https://gist.github.com/dehowell/884204?permalink_comment_id=1771089#gistcomment-1771089\n",
    "def url_is_alive(url):\n",
    "    request = urllib.request.Request(url)\n",
    "    request.get_method = lambda: 'HEAD'\n",
    "    try:\n",
    "        urllib.request.urlopen(request)\n",
    "        return True\n",
    "    except urllib.request.HTTPError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your function with the following code, all the url's should be valid if your function works properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musicians = ['Akon ', 'taylor swift', ' Bob dylan', ' Cardi B ']\n",
    "url_root = 'https://en.wikipedia.org/wiki/'\n",
    "\n",
    "for musician in musicians:\n",
    "    page_name = name_to_wikipedia_page_name(musician)\n",
    "    target_url = url_root + page_name\n",
    "    is_real_url = url_is_alive(target_url)\n",
    "    print(f'URL: {target_url} is {\"valid\" if is_real_url else \"not valid\"}')\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Add functions to chatbot\n",
    "\n",
    "Add the functions `get_html_content` `name_to_wikipedia_page_name` and `url_is_alive` to the class `MusicBot` as member functions, don't forget to use the `self` keyword here. \n",
    "\n",
    "### Task 7: Add code to constructor \n",
    "\n",
    "Add the code to [create a requests session object and limit the requests made by the session](#limit-requests) to the constructor (`__init__`) of class `MusicBot`. Don't forget to use the `self` keyword to make the `sessions` object a member variable of your `MusicBot` class.\n",
    "\n",
    "### Task 8: Write a function to get biographical information about a musician\n",
    "\n",
    "Write a function that gets biographical information from the URL [using the examples above](#extract-biographical-information-from-the-html). There are many things you can get, such as age, birthplace, and musical genres. You can use the function below as a template. \n",
    "> Tip: you may want to use a [Try-Except block](https://www.w3schools.com/python/python_try_except.asp) in your function to catch any instances where the data you are looking for cannot be found on a particular page, then return something useful so that you can tell the user that information can't be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age(url):\n",
    "    # Get HTML content from URL\n",
    "    # Extract the age from the HTML\n",
    "    # Return the age as an Int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have written it, test your function and then add it to the class `MusicBot` as member function. \n",
    "\n",
    "### Task 9: Create chat interface for retrieving biographical information\n",
    "\n",
    "In the function `generate_response`:\n",
    "1. Use a regex to match and input from the user where they ask a question about their favourite musician, i.e. the string input 'how old is {x}'. {x} will be the name of the musician.\n",
    "2. Use the function `name_to_wikipedia_page_name` to get the URL for the specified musician.\n",
    "3. Check to see if the url is valid using the function `url_is_alive`, if not tell the user the musician could not be found, if it is then move onto the next step.\n",
    "4. Pass the url into your function that retrieves information about the artist.\n",
    "5. Return a response that includes the artist name and the information that has been extracted.\n",
    "\n",
    "\n",
    "## Bonus tasks\n",
    "\n",
    "Some more bonus tasks...\n",
    "\n",
    "#### Bonus task A\n",
    "\n",
    "Write functions for [all of the biographical information](#extract-biographical-information-from-the-html) that can be found from the wikipedia page. Then incorporate them into the function `generate_response` in your chatbot.\n",
    "\n",
    "#### Bonus task B\n",
    "\n",
    "1. Look at the wikipedia page for a musician. Is there any other information that you could extract for your chatbot? \n",
    "2. Look at the source of the wikipedia page to see if you can identify the necessary tags to extract that content. \n",
    "3. Then try writing some code in this notebook to see if you can reliably extract that information.\n",
    "4. Then follow the previous steps to incorporate that into your chatbot.\n",
    "\n",
    "#### Bonus task C\n",
    "\n",
    "Add functionality to your chatbot that allows it to answer questions about bands as well as solo musicians.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
