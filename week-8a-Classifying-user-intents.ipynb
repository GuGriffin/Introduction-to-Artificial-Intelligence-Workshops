{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8a: Classifying user intents\n",
    "\n",
    "This week you will be learning about how to train a text classifier, that will allow you build chatbots that can recognise and respond to user intents. \n",
    "\n",
    "To do this you will be training a text classification algorithm, [using the library SetFit](https://github.com/huggingface/setfit) that can be used to train powerful text classifiers with extremely small amounts of data. In this example you will be [using a dataset with three examples per class](class-datasets/basic_intents_train.csv) (a tiny amount of data!!!). **Because this can be trained with so little data, this is a perfect tool for you to adapt to building chatbots for your own projects!** \n",
    "\n",
    "The code in this notebook is adapted from: https://hackernoon.com/mastering-few-shot-learning-with-setfit-for-text-classification\n",
    "\n",
    "Before you get started though, let just make sure that this notebook is setup to run using the `nlp` conda environment that you created at the start of term.\n",
    "\n",
    "To set this notebook to the right environment, click the **Select kernel** button in the top right corner of this notebook, then select **Python Environments...** and then select the environment `nlp`.\n",
    "\n",
    "To double check you have done this correctly, hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can import the libraries that you will be using for the activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset \n",
    "\n",
    "This code loads in two datasets, the **training** dataset and the **testing** dataset. \n",
    "\n",
    "[The training dataset](class-datasets/basic_intents_train.csv) is what is used to iteratively improve the classifier model. In effect, this is the data that the model 'practices' on and learns from through trial and error.\n",
    "\n",
    "[The testing dataset](class-datasets/basic_intents_train.csv) is the what we use to evaluate the performance of the classifier after training. It is very important when training a model that you do not train on your testing data. The test data needs to remain unseen by the model, so that you can get an accurate accuracy score for how the model performs on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('csv', data_files={\n",
    "    \"train\": 'class-datasets/basic_intents_train.csv',\n",
    "    \"test\": 'class-datasets/basic_intents_test.csv'\n",
    "})\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "intent_dataset_train = le.fit_transform(dataset[\"train\"]['label'])\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(\"label\").add_column(\"label\", intent_dataset_train).cast(dataset[\"train\"].features)\n",
    "\n",
    "intent_dataset_test = le.fit_transform(dataset[\"test\"]['label'])\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(\"label\").add_column(\"label\", intent_dataset_test).cast(dataset[\"test\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and train classifier\n",
    "\n",
    "This code creates the classifier object which is called `model` because it is a *statistical model* of the training data. \n",
    "\n",
    "the object `trainer` is the code that trains the model, by iterating over data and updating the *model parameters* (aka weights) in the classifier, we can configure the training process by passing in different training *hyperparameters* such as `loss_class`, `batch_size`, `num_iterations` and `num_epochs`. \n",
    "\n",
    "The line of code `trainer.train()`, is where the training happens, depending on the size of your dataset this can take a long time, however with such small training datasets as we have here it should not take too long. After training the model is evaluated on the test set with `trainer.evaluate()` and then the weights of the trained model are saved, so that you can use them in the chatbot [week-13c-intent-based-chatbot.py](week-13c-intent-based-chatbot.py).\n",
    "\n",
    "Don't worry if you don't understand everything that is going on here at the moment, what happens under the hood when machine learning models like this are trained is covered in depth next term in **AI for Media**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and trainer\n",
    "model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SetFitModel.from_pretrained(model_id)\n",
    "\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    metric=\"accuracy\",\n",
    "    batch_size=64,\n",
    "    num_iterations=20,\n",
    "    num_epochs=2,\n",
    "    column_mapping={\"text\": \"text\", \"label\": \"label\"}\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)\n",
    "\n",
    "os.makedirs('ckpt/', exist_ok=True)\n",
    "\n",
    "trainer.model._save_pretrained(save_directory=\"ckpt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class label mapping\n",
    "\n",
    "The classifer will only learn to predict integer values for each class, for us to understand what class each number represents you need to manually keep track of them in some-kind of data structure:\n",
    "\n",
    "> Note: If you adapt this code for your projects and add or change these classes, then you will need to change this data structure as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_map = {\n",
    "    0: \"Greeting\",\n",
    "    1: \"Farewell\",\n",
    "    2: \"Positive Confirmation\",\n",
    "    3: \"Negative Confirmation\",\n",
    "    4: \"Small Talk\",\n",
    "    5: \"Time Enquiry\",\n",
    "    6: \"Help\",\n",
    "    7: \"Escalation to Human\",\n",
    "    8: \"Request Joke\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test classifier on new input\n",
    "\n",
    "Now you can test the model using `model.predict()` on a new string input. \n",
    "\n",
    "> Note: This functions returns [PyTorch Tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html) (we will be covering exactly what this is in AI for Media next term). This contains an integer that represents the most likely the class index. You can simply cast this as a Python `Int` to get the value in a variable that is easier to work with in Python.\n",
    "\n",
    " Try changing the value of `input_text` to get predictions for some different classes, try using a different expression or wording to what is given in [the training data](class-datasets/basic_intents_train.csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output class: 8, which is intent: 'Request Joke'\n"
     ]
    }
   ],
   "source": [
    "model = SetFitModel.from_pretrained(\"ckpt/\", local_files_only=True)\n",
    "\n",
    "input_text = \"I need a good laugh\"\n",
    "output = model.predict(input_text)\n",
    "output_label = int(output)\n",
    "\n",
    "print(f\"Predicted output class: {output_label}, which is intent: '{class_label_map[output_label]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above (`model.predict()`) will only tell you the index of the class with the highest confidence probability. Often this is all that you would care about, but sometimes you might want to see what the confidence score is for both the highest rating and the other classes. \n",
    "\n",
    "To get that information you can call the function `model.predict_proba()`, which will give an array of floats containing the confidence probabilities for every class in the training dataset.\n",
    "\n",
    "> Note: This functions also returns [PyTorch Tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html). To convert this into a nice list of Python `Float` variables, you can call the function [.tolist()](https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html).\n",
    "\n",
    "This output array only contains the confidence probabilities for each class, the index of the classes is given by the order in the list.\n",
    "\n",
    "If you want to find out the maximum value and the class index label you need to use functions `max` to get the maximum value in the list and `list.index` to then find the index for the maximum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of class probabilities: [0.06979068512279166, 0.07797967909695576, 0.07026228583253973, 0.06973780979281664, 0.08603421715207447, 0.07036901482089156, 0.07528835358312502, 0.07648611531165186, 0.4040518392871534]\n",
      "Predicted class: 8 with confidence 0.4041 which is intent 'Request Joke'\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I need a good laugh\"\n",
    "output_probs = model.predict_proba(input_text)\n",
    "output_probs = output_probs.tolist()\n",
    "print(f'list of class probabilities: {output_probs}')\n",
    "\n",
    "max_conf = max(output_probs)\n",
    "max_class = output_probs.index(max_conf)\n",
    "\n",
    "print(f\"Predicted class: {max_class} with confidence {max_conf:.4f} which is intent '{class_label_map[max_class]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "**Task 1:** Run all the cells in this notebook to [train](#setup-and-train-classifier) and [test](#test-classifier-on-new-input) your classifier.\n",
    "\n",
    "**Task 2:** [week-8c-Intent-based-chatbot.py](week-8c-intent-based-chatbot.py) has functions already made that respond to all of the intent classes in the dataset. Write some code in `generate_response` that takes the string `processed_input`, feeds it into the classification model and gets the predicted class for the input. Based on the predicted class (0-9) use this to respond with the relevant function based on each intent.\n",
    "\n",
    "> Tip: There is a comment above each function describing what intent each function responds too. Use this information and the [class label map](#class-label-mapping) to determine which function needs to be called depending on the class prediction.\n",
    "\n",
    "That's it! Now you can move onto the tasks in [Week-8b-Sentiment-analysis.ipynb](Week-8b-Sentiment-analysis.ipynb). After that feel free to come back to the bonus tasks in here, or you could start adapting the dataset in this classifier to be relevant for your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus tasks\n",
    "\n",
    "**Task A:** Try adding a new intent to the this dataset. Such as asking todays date, asking the chatbot what it's name is, or what it's favourite colour is. Add 3 examples for user intents to the dataset and a new class label, e.g. `10`. After you have trained the model, test it. If it works, add a function to the `IntentChatbot` that responds to the intent and then add this new functionality to the chatbot. \n",
    "\n",
    "**Task B:** In `generate_response` use the function `predict_proba` to get the confidences of the predictions from the classifier. If the max prediction is below a certain threshold, i.e. 0.3, then instead of responding based on the max prediction, give a response saying something such as 'I do not understand'. Try out different values for this threshold to see if you can get the best tradeoff between accurately responding to the intents in the dataset but recognising when an input is not in one of those categories and responding with a different response. \n",
    "\n",
    "**Task C:** Create a new dataset to adapt this classifier to intents (or other categories you may want to classify based on user inputs) that are relevant to your project. Train the model, test it, and then try importing into the chatbot code for your projects. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
